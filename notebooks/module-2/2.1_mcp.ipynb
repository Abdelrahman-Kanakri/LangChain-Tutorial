{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "#from langchain_mistralai import ChatMistralAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") \n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'extras': {'signature': 'Co8JAXLI2ny4Es7Hms5EmF8Ggj7S/wJAW6wNfOkzmN81BjITqTGc9lWa3nEs5v5aoS5g4EVZJW1iCKY13fmosPVuGDvx8NojQiBr1u/zU2DNSjatbIbAyZdrf90nwMve9KtzmRaYNG4UIcFRSUX8e7nxWrFEmQVTrQ3IcLtBE4ltxbtHvI86B7llGec2tdRp/q2Y1EhtcnBcDCGFHzOnNTEHBhYs76vPiDhJoWmG0TEnwkawBF0N5jG84p82SeCG1nT2C5/ZhsUlP5cfOEFcdnqhY2ymhRnukUAZq/ot1M4we6hrRKTxjsQTlu+tbhdZ3E+na7TDXaSUfzoIkoDmJBTdA5IIqwynhWw+xHakbQX7367doPJ8SGAwGW9tZ7w3oL20r9o9nUFZL/6+vyI6DyKMfkN2bmyUuc9sINGohfUB4s1xlKL8LkKnLRWQpFaXapaxPSlHjqSDIyfntfDU+bUx3xceR9PxcOOEFPVJKhCN/HF7LDFhcnwgsp99kAiup7Z60h33RmWJIWEYD5BRtPgtezvYN3l1WRvLlreHBRMLa/zYkSqgdKAVxo7xz3LFzUwp8gysC6gevB3PGtaHSMcKd1s9Fy+8XuYf4TXusHGQ9bIEU1BeO+iOM49QAWbRjbiYlZQg7UE3WMP7UiUYKlNhefcZBpyOovd3jHjVEzYe1i4XTc9uF52BrwIOJ2CBngOuJE/FWnZemgikHUWvrE6YyrJqLNpimCdFoF+PhxPZv0jmsjqBgaNtQ6w4HHNMGpYx0yyxSWRTEj1r066o4HD9ZtRGNcIq/N/70t1wMwaYv3tYoMaGCJs2E0Tux+cjoHP5BJYMlUGgL2hzVq3rxhLPUhmj05rh8XZ9CxgOjr1PQNIq571Z/+EigNNhnUVzz9u5D/OKAQMyKCB/ejZg/Yo6CmeEiPUqHD/w86HK9s4Z+kfjdQcnzfplE9M25uoN/4blwxEWVCNRQMRsZ5ijhJ/aKnykB0Q5SSvQ0dG9jdFDzeVEzyP1sv6wa+DsBE6ATVoQ313YGw+2IrVra8FGLxJFs4Y+Bc/JUYbEoJuIJAvCLCY6GjYS+ate5kVCl51YMYZGk/Nj36zTtAOlDLw2NdToOrzzMwyXTYdXQK+cN24aqCPF53tYEfXV8Gkbh5CD+sgak+juyIWeMOdXKOc67FdUEkkyA3TWnFpQ8qNF3zddQK2DPD1qsPP9Mf2LeGmQfy0qH8AK6S2aD86TnySqkiJ0atly2TKWR2QOBBQqeO4gmt7kD9Xf+bGOT/8XQpSrUxoJsxxhDU2v53gzXLnc8Xg+ecnD1BjznIajxZt/vV11IbnLmSDJQHDu1w/zPp1rI+oO5D7iOdoTaZW8Um4Jx64MJ6xuyuGo2RfT+0rLqyRIMahS2v+lKhiCIASY1BQBR3C50P8N+MCo5UuT5+TmQzylOYeQ9J7630i4OJ5Rfs97+G3i2iOYvbyyFmUdNmh1lyEZXD/L9qT9bdwzx21e0E+tLO3W9xPw8QbFPKkWOui0kWbqXe9uZOyhYp4rrVgnp8+A/ZZBfKENvU6G5dc/lPDv'},\n",
      "  'text': 'The `langchain-mcp-adapters` library provides a lightweight wrapper '\n",
      "          'that makes Anthropic Model Context Protocol (MCP) tools compatible '\n",
      "          'with LangChain and LangGraph. It also extends this compatibility to '\n",
      "          'LangChain.js.\\n'\n",
      "          '\\n'\n",
      "          'Key functionalities and benefits include:\\n'\n",
      "          '\\n'\n",
      "          '*   **Tool Conversion:** It converts MCP tools into a format that '\n",
      "          'is compatible with LangChain and LangGraph.\\n'\n",
      "          '*   **Multi-Server Interaction:** The library enables interaction '\n",
      "          'with tools hosted on multiple MCP servers.\\n'\n",
      "          '*   **Seamless Integration:** It allows for the seamless '\n",
      "          'integration of numerous existing MCP tool servers into LangGraph '\n",
      "          'Agents.\\n'\n",
      "          '*   **Simplified Connectivity:** It simplifies the process of '\n",
      "          'connecting LangChain and LangGraph to the expanding ecosystem of '\n",
      "          'MCP tool servers, eliminating the need for manual adaptation of '\n",
      "          'each tool.\\n'\n",
      "          '*   **Enhanced Agent Capabilities:** Agents can leverage tools from '\n",
      "          'multiple MCP servers simultaneously, facilitating more powerful and '\n",
      "          'versatile applications.\\n'\n",
      "          '\\n'\n",
      "          'The core class for managing these connections and loading tools, '\n",
      "          'prompts, and resources from MCP servers is `MultiServerMCPClient`. '\n",
      "          'This library essentially standardizes how applications provide '\n",
      "          'tools and context to Large Language Models (LLMs), allowing '\n",
      "          'LangChain agents to readily utilize tools defined on various MCP '\n",
      "          'servers.',\n",
      "  'type': 'text'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient( # for the mcp, I can look for any mcp in the web, then get this json structure, and create a client for it\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5568296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='get_current_time', description='Get current time in a specific timezones', args_schema={'type': 'object', 'properties': {'timezone': {'type': 'string', 'description': \"IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Use 'America/New_York' as local timezone if no timezone provided by the user.\"}}, 'required': ['timezone']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001EBE3CE7600>),\n",
       " StructuredTool(name='convert_time', description='Convert time between timezones', args_schema={'type': 'object', 'properties': {'source_timezone': {'type': 'string', 'description': \"Source IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Use 'America/New_York' as local timezone if no source timezone provided by the user.\"}, 'time': {'type': 'string', 'description': 'Time to convert in 24-hour format (HH:MM)'}, 'target_timezone': {'type': 'string', 'description': \"Target IANA timezone name (e.g., 'Asia/Tokyo', 'America/San_Francisco'). Use 'America/New_York' as local timezone if no target timezone provided by the user.\"}}, 'required': ['source_timezone', 'time', 'target_timezone']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001EBE3CE77E0>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    #model=\"gpt-5-nano\",\n",
    "    model, # \"gemini-flash-2.5\"\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='c2d4f26a-08d3-4a1b-ad48-3c87380f5670'),\n",
      "              AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_time', 'arguments': '{\"timezone\": \"America/New_York\"}'}, '__gemini_function_call_thought_signatures__': {'cdd1548e-ec8b-4e80-97dd-e9c29404a5e5': 'CrwCAXLI2ny+90hF1p1QLX2uQtkngmFGFG/CmJmTNWYbHQ/IdP0AuxJgLfGQOCNwrUQOuHNnI5dkQIinZG/axKlw79gwuMw37SHTasPhRj50gXnrKdtrvQ4hPznXj4CxjgFU264tFIAADCAX7lF2zF63KbqLkraO1A3ivwTizz5YOSdPLtN7OS/QHky2YQJ75bYKUAL5ahXoMp55KcaDCubeJff20W/zznqt/mmrOf4UiCg5il3bNRfbVRXC5T2Q41aAXedy0HYKdCvioF/81iD09C/Gk8i31cRmAhRJzd7VsF+LghqSo9cO09oFgCqSuOBQbd0G+4QRyXZvrmyZjRToYA8a8OLhXxDKD2uw7fkQMGAUiLzj22LLIzj6RVbBacuYmBe3pI0NU1aZrt0+xCW5CXKa14TDk+PvdhJnog=='}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b9804-e358-7712-80ec-2e7a72b4945e-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'cdd1548e-ec8b-4e80-97dd-e9c29404a5e5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 250, 'output_tokens': 88, 'total_tokens': 338, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 67}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-01-07T05:33:37-05:00\",\\n  \"day_of_week\": \"Wednesday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_13cd2580-54b6-4695-9f0a-e9b5b9a392e6'}], name='get_current_time', id='9700cd17-866f-44b9-9ba4-c8b8161cdc42', tool_call_id='cdd1548e-ec8b-4e80-97dd-e9c29404a5e5'),\n",
      "              AIMessage(content='The current time in America/New_York is 05:33:37 on Wednesday, January 7, 2026.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b9804-ebef-76f3-967d-7ea1c5602fc8-0', usage_metadata={'input_tokens': 410, 'output_tokens': 32, 'total_tokens': 442, 'input_token_details': {'cache_read': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
